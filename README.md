# Overview
This project takes a deep dive into training and comparing the outputs generated by different segmentation models.
<br>
The segmentation models that will be used are:
| Model    | Performance |
| -------- | ------- |
| UNET  | -    |
| Autoencoder | -     |
| CLIP    | -    |
| Prompt-based    | -    |

## What is image segmentation?
[Video Explanation](https://www.youtube.com/watch?v=5QUmlXBb0MY)

In an image, there can be different objects such as the sky, trees and people.

An image segmentation simply divides the image into different regions.

A semantic segmentation gives a class to each pixel in the image.

Take a look at this cat image.
![image](Abyssinian_1_color.jpg)
The semantic mask is as follows.
![image](Abyssinian_1_mask.png)

| Object    | Semantic Color |
| -------- | ------- |
| cat  | red with white outline   |
| couch & wall | black     |
## Model Training
We will be performing supervised training on the models to allow them to extract the features from the image and correctly map the objects to their semantic classes (colors).


